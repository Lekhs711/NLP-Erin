{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Find a .txt file on Project Gutenberg to download using request.urlopen()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http.client.HTTPResponse object at 0x1a13a62860>\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\r\\nThe Project Gutenberg EBook of The Strange Case Of Dr. Jekyll And Mr.\\r\\nHy'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/43/43-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "print(response)\n",
    "raw = response.read().decode('utf-8-sig')\n",
    "print(type(raw))\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the text file and use word_tokenize to create a list of the word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'The',\n",
       " 'Strange',\n",
       " 'Case',\n",
       " 'Of',\n",
       " 'Dr.',\n",
       " 'Jekyll',\n",
       " 'And',\n",
       " 'Mr.',\n",
       " 'Hyde',\n",
       " ',']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize \n",
    "tokens = word_tokenize(raw)\n",
    "print(type(tokens))\n",
    "tokens[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Find the points in the book that contain the Project Gutenberg information and remove that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before deleting: \r\n",
      "The Project Gutenberg EBook of The Strange Case \n",
      "Pattern to be removed: project gutenberg\n",
      "Patterns deleted: 71\n",
      "Text after deleting: \r\n",
      "The  EBook of The Strange Case Of Dr. Jekyll And\n"
     ]
    }
   ],
   "source": [
    "# remove 'Project Gutenberg' from text\n",
    "print('Text before deleting:',raw[:50])\n",
    "word = 'project gutenberg'\n",
    "word_len = len(word)\n",
    "print('Pattern to be removed:',word)\n",
    "start_idx = 0\n",
    "count = 0\n",
    "raw_clean = raw\n",
    "\n",
    "while start_idx != -1:\n",
    "    start_idx = raw_clean.lower().find(word)\n",
    "    raw_clean = raw_clean[:start_idx] + raw_clean[start_idx+word_len:]\n",
    "    count += 1\n",
    "print('Patterns deleted:',count)\n",
    "print('Text after deleting:',raw_clean[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to your favorite news organization and find an article you want to use as a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.cnn.com/2019/02/10/us/denver-teacher-strike-multiple-jobs/index.html\"\n",
    "html = request.urlopen(url).read().decode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import that file using bs4 package and HTML format clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(html).get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Living',\n",
       " 'with',\n",
       " 'roommates',\n",
       " 'and',\n",
       " 'working',\n",
       " 'multiple',\n",
       " 'jobs',\n",
       " ',',\n",
       " 'here',\n",
       " \"'s\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Using your book or your html file, print a list of all wh word types that occur (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on).\n",
    "<br>\n",
    "– Use the set function to get only a unique list of these words.\n",
    "<br>\n",
    "– You do not have to use .islower to clean it up, let’s look at the raw list to see how much variability\n",
    "there is in text.\n",
    "<br>\n",
    "– Are any words duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what\n",
      "whatever\n",
      "whatsoever\n",
      "whatsoever.You\n",
      "wheel—if\n",
      "when\n",
      "whenever\n",
      "where\n",
      "whereabouts\n",
      "whereas\n",
      "wherein\n",
      "wherewithal\n",
      "whether\n",
      "whetted\n",
      "which\n",
      "while\n",
      "whilst\n",
      "whimpering\n",
      "whipped\n",
      "whipping\n",
      "whisper\n",
      "whispered\n",
      "whispering\n",
      "whisper—\n",
      "white\n",
      "whither\n",
      "who\n",
      "whole\n",
      "wholesale\n",
      "wholly\n",
      "whom\n",
      "whose\n",
      "why\n",
      "Unique words starting with wh: 33\n"
     ]
    }
   ],
   "source": [
    "text = raw\n",
    "words = nltk.word_tokenize(text)\n",
    "wh_list = sorted(set([w for w in words if re.search(r'^wh', w)]))\n",
    "for word in wh_list:\n",
    "    print(word)\n",
    "print('Unique words starting with wh:',len(wh_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, \"whisper\" and \"whisper-\" would be same if we removed punctuations\n",
    "All words are lower, so adding .islower would not make any difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append ay, e.g. string → ingstray, idle → idleay. http://en.wikipedia.org/wiki/Pig_Latin\n",
    "<br>\n",
    "– Write a function to convert a word to Pig Latin.\n",
    "<br>\n",
    "– Use regular expressions to find the first vowel in a word, index that, and then rearrange the word\n",
    "to be vowel to end + beginning + ay.\n",
    "<br>\n",
    "– Test your function on the following words: cheese, elephant, moose, thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eesechay\n",
      "elephantay\n",
      "oosemay\n",
      "ingthay\n"
     ]
    }
   ],
   "source": [
    "def get_pig_latin(word):\n",
    "    \"\"\"Returns the pig latin version of the word\"\"\"\n",
    "    idx_vowel = (re.search('[aeiou]', word)).start()\n",
    "    pig_latin = word[idx_vowel:] + word[:idx_vowel] + 'ay'\n",
    "    return pig_latin\n",
    "\n",
    "print(get_pig_latin('cheese'))\n",
    "print(get_pig_latin('elephant'))\n",
    "print(get_pig_latin('moose'))\n",
    "print(get_pig_latin('thing'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Use the Porter Stemmer to normalize your tokenized book or html document, calling the stemmer on each word.\n",
    ">Do the same thing with the Lancaster Stemmer and see if you observe any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strang',\n",
       " 'case',\n",
       " 'Of',\n",
       " 'dr.',\n",
       " 'jekyl',\n",
       " 'and',\n",
       " 'mr.',\n",
       " 'hyde',\n",
       " ',',\n",
       " 'by',\n",
       " 'robert',\n",
       " 'loui',\n",
       " 'stevenson',\n",
       " 'thi']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(t) for t in tokens][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strange',\n",
       " 'cas',\n",
       " 'of',\n",
       " 'dr.',\n",
       " 'jekyl',\n",
       " 'and',\n",
       " 'mr.',\n",
       " 'hyd',\n",
       " ',',\n",
       " 'by',\n",
       " 'robert',\n",
       " 'lou',\n",
       " 'stevenson',\n",
       " 'thi']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lancaster.stem(t) for t in tokens][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, there are differences between the two stemmers we can see that the word strange has \"strang\" with and e in lancaster stemmer whereas in potter there is no \"e\". Case is missing an in lancaster stemmer. Hyde is missing an \"e\" in lancaster stemmer. Loui is missing an \"i\" in lancaster stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "<br>Using your book or html file, tokenize the document into both words (word_tokenize) and sentences (sent_tokenize).\n",
    "<br>Calculate the average length of the words avg_w and the average length of the sentences avg_s\n",
    "<br>Calculate a readability index by using the formula: 4.71*avg_w + .5*avg_s - 21.43."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokens\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "# Sentence tokens\n",
    "sents = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 3.781900372794787\n",
      "Average sentences length: 148.55317164179104\n",
      "Readability Index: 70.65933657675896\n"
     ]
    }
   ],
   "source": [
    "word_len_list = [len(w) for w in tokens]\n",
    "avg_w = sum(word_len_list)/len(word_len_list)\n",
    "\n",
    "sents_len_list = [len(s) for s in sents]\n",
    "avg_s = sum(sents_len_list)/len(sents_len_list)\n",
    "\n",
    "print('Average word length:',avg_w)\n",
    "print('Average sentences length:',avg_s)\n",
    "\n",
    "readability_idx = 4.71*avg_w + 0.5*avg_s - 21.43\n",
    "print('Readability Index:',readability_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
